"""
ğŸ§© Cross Validation (CV) æ ¸å¿ƒé€»è¾‘æ€»ç»“

CV çš„æ ¸å¿ƒé€»è¾‘æ˜¯åœ¨ (X_train, y_train) ä¸ŠæŠŠæ•°æ®åˆ’åˆ†ä¸º K ä¸ª foldsï¼Œ
åœ¨ K æ¬¡å¾ªç¯ä¸­ï¼Œæ¯æ¬¡éƒ½ä½¿ç”¨ä¸åŒçš„ fold ä½œä¸º validation setï¼Œ
å‰©ä¸‹çš„ folds åˆå¹¶ä¸º training setã€‚è¿™æ ·æ¨¡å‹åœ¨æ¯æ¬¡è®­ç»ƒæ—¶
éƒ½èƒ½åœ¨ä¸åŒçš„æ•°æ®åˆ†å¸ƒä¸‹è¢«è¯„ä¼°ã€‚

ğŸ’¡ ç›®çš„ï¼š
é€šè¿‡å¤šæ¬¡è®­ç»ƒâ€“éªŒè¯ï¼Œå¯ä»¥æ›´ç¨³å®šåœ°ä¼°è®¡æ¨¡å‹çš„
generalization abilityï¼ˆæ³›åŒ–èƒ½åŠ›ï¼‰ï¼Œ
è€Œä¸æ˜¯ä¾èµ–äºæŸä¸€æ¬¡éšæœºåˆ’åˆ†å¯èƒ½ä¸å‡è¡¡çš„ç»“æœã€‚

ğŸ“ˆ åŸç†ä¸Šï¼š
æ¯ä¸€æ¬¡ fold ä»£è¡¨æ¨¡å‹åœ¨ä¸åŒæ•°æ®ä¸Šçš„è¡¨ç°ã€‚
- æœ‰çš„ fold å¯èƒ½ fit ä¸è¶³ï¼ˆbias è¾ƒé«˜ï¼‰
- æœ‰çš„ fold å¯èƒ½ fit è¿‡åº¦ï¼ˆvariance è¾ƒé«˜ï¼‰
æŠŠå®ƒä»¬çš„å¹³å‡ç»“æœä½œä¸ºæœ€ç»ˆä¼°è®¡ï¼Œ
å¯ä»¥æ›´å…¨é¢åœ°åæ˜ æ¨¡å‹çš„æ•´ä½“ biasâ€“variance balanceã€‚

âš™ï¸ å› æ­¤ï¼ŒCV å¸¸ä¸ Grid Search ä¸€èµ·ä½¿ç”¨ï¼š
- å¤–å±‚ Grid Searchï¼šå¾ªç¯ä¸åŒçš„æ¨¡å‹å‚æ•°ï¼ˆhyperparametersï¼‰
- å†…å±‚ Cross Validationï¼šè¯„ä¼°å½“å‰å‚æ•°ä¸‹æ¨¡å‹çš„å¹³å‡éªŒè¯æ€§èƒ½

æœ€ç»ˆé€‰æ‹©åœ¨æ‰€æœ‰å‚æ•°ç»„åˆä¸­ï¼ŒCV å¹³å‡è¡¨ç°æœ€å¥½çš„é‚£ç»„è¶…å‚æ•°ï¼Œ
æ—¢èƒ½å¹³è¡¡ bias ä¸ varianceï¼Œåˆèƒ½æœ€å¤§åŒ–æ³›åŒ–æ€§èƒ½ã€‚
"""

from regression_glm import *
from sklearn.datasets import make_classification
from itertools import product


def get_k_fold(X_train, k, random_state):
    """
    1. Get the total numbers of X, m
    2. Randomize the index
    3. split the index into k folds
    list<list<indexs>>
    4. for each k fold, iterate K time, each loop get 1 fold to be validation set, and the rest is the training
    5. return list<tuple(train, val)>
    """

    m = X_train.shape[0]
    np.random.seed(random_state)

    m_index = list(range(m))
    np.random.shuffle(m_index)

    # now split the index into folds using np.array_split
    folds = np.array_split(m_index, k)

    splits = []
    for i in range(k):
        this_val_index = folds[i]
        this_train_index = np.setdiff1d(m_index, this_val_index)

        splits.append((this_train_index, this_val_index))

    return splits


def k_fold_cross_validation(X, y, k, random_state, evaluation_metrics, model, **paremeters):
    """
    The input is the X_train actually, because you must leave the test set to be independent
    """
    splits = get_k_fold(X, k, random_state)

    performances = []
    for split in splits:
        train_index, val_index = split
        X_train, X_val = X[train_index], X[val_index]
        y_train, y_val = y[train_index], y[val_index]

        this_model = model(X_train, y_train, **paremeters)
        this_model.fit()
        y_pred = this_model.predict(X_val)

        performance = evaluation_metrics(y_val, y_pred)
        performances.append(performance)
    return np.mean(performances), np.std(performances)


def grid_search(X, y, k, random_state, evaluation_metrics, model, parameter_grid):
    """
    paremeter grid is dict(list(parameters))
    """
    all_parameter_keys = list(parameter_grid.keys())  # list of parameter names

    # gives you all the combinations a by b, suppose only 2 paremeters, and each length is a and b, then a by b combinations
    all_combinations = list(product(*parameter_grid.values()))

    best_performance = 0  # if our evaluation is accuracy
    best_setup = None
    for combination in all_combinations:
        the_paremeter_dict = dict(zip(all_parameter_keys, combination))  # since the dim of key and value is the same
        the_paremeter_dict["which_regression"] = "logistic"
        this_set_up_performance_via_kfold = k_fold_cross_validation(X, y, k, random_state, evaluation_metrics, model,
                                                                    **the_paremeter_dict)
        print(this_set_up_performance_via_kfold)

        if float(this_set_up_performance_via_kfold[0]) > best_performance:
            best_performance = this_set_up_performance_via_kfold[0]
            best_setup = the_paremeter_dict

    return best_performance, best_setup


if __name__ == '__main__':
    X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0)
    parameters = {"reg_lambda": 0.5, "lr": 0.01, "which_regression": "logistic"}
    # model = RegressionGLM(X, y, **parameters)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    this_set_up_performance_via_kfold = k_fold_cross_validation(X_train, y_train, 5,
                                                                random_state=42, evaluation_metrics=accuracy,
                                                                model=RegressionGLM,
                                                                **parameters)

    parameter_grid = {"lr": [0.1, 0.01, 0.001], "reg_lambda": [0.1, 0.5, 1]}

    best_performance = grid_search(X_train, y_train, 5,
                                   random_state=42, evaluation_metrics=accuracy,
                                   model=RegressionGLM,
                                   parameter_grid=parameter_grid)
