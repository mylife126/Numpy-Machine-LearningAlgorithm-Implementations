ğŸ§© 1ï¸âƒ£ Linear Regression ï¼ˆæœ€åŸºæœ¬æ¨¡å‹ï¼‰

ğŸ§® Lossï¼ˆMSEï¼‰

$J(\theta) = \frac{1}{2m}\|X\theta - y\|^2$
æˆ–
$J(\theta) = \frac{1}{2m}(X\theta - y)^T(X\theta - y)$

âš™ï¸ Gradient

$\nabla_\theta J = \frac{1}{m} X^T(X\theta - y)$

âœ… è¯´æ˜ï¼š
	â€¢	ä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å°åŒ–å‡æ–¹è¯¯å·®ï¼›
	â€¢	å¯ä»¥ç”¨è§£æè§£ $\theta = (X^TX)^{-1}X^Ty$ï¼Œ
ä¹Ÿå¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™ï¼›
	â€¢	æŸå¤±å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼Œæ‰€ä»¥ GD ä¸€å®šæ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ã€‚

â¸»

ğŸ§© 2ï¸âƒ£ Ridge Regression ï¼ˆL2 æ­£åˆ™åŒ–çº¿æ€§å›å½’ï¼‰

ğŸ§® Loss

$J(\theta) = \frac{1}{2m}\|X\theta - y\|^2 + \frac{\lambda}{2m}\|\theta\|^2$

âš™ï¸ Gradient

$\nabla_\theta J = \frac{1}{m}X^T(X\theta - y) + \frac{\lambda}{m}\theta$

âœ… è¯´æ˜ï¼š
	â€¢	é¢å¤–é¡¹ $\frac{\lambda}{2m}\|\theta\|^2$ æ˜¯ L2 æ­£åˆ™ï¼›
	â€¢	å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½¿æƒé‡å˜å°ï¼›
	â€¢	è§£æè§£ä¸º $\theta = (X^TX + \lambda I)^{-1}X^Ty$ã€‚

â¸»

ğŸ§© 3ï¸âƒ£ Lasso Regression ï¼ˆL1 æ­£åˆ™åŒ–çº¿æ€§å›å½’ï¼‰

ğŸ§® Loss

$J(\theta) = \frac{1}{2m}\|X\theta - y\|^2 + \frac{\lambda}{m}\|\theta\|_1$

âš™ï¸ Gradient / Subgradient

$\nabla_\theta J = \frac{1}{m}X^T(X\theta - y) + \frac{\lambda}{m}\,\text{sign}(\theta)$

âœ… è¯´æ˜ï¼š
	â€¢	L1 æ­£åˆ™é¡¹çš„å¯¼æ•°ä¸æ˜¯è¿ç»­çš„ â†’ ç”¨ subgradientï¼ˆç¬¦å·å‡½æ•° signï¼‰ï¼›
	â€¢	ä¼šäº§ç”Ÿç¨€ç–è§£ï¼ˆéƒ¨åˆ†æƒé‡å˜ 0ï¼‰ï¼›
	â€¢	å¸¸ç”¨ä¼˜åŒ–å™¨ï¼šåæ ‡ä¸‹é™ (Coordinate Descent) æˆ– Proximal GDã€‚

â¸»

ğŸ§© 4ï¸âƒ£ Logistic Regression ï¼ˆäºŒåˆ†ç±»ï¼‰

ğŸ§® Sigmoid

$\hat{p} = \sigma(X\theta) = \frac{1}{1 + e^{-X\theta}}$

ğŸ§® Lossï¼ˆCross-Entropy / Negative Log Likelihoodï¼‰

$J(\theta) = -\frac{1}{m}\sum_i [y_i \log(\hat{p_i}) + (1 - y_i)\log(1 - \hat{p_i})]$

âš™ï¸ Gradient

$\nabla_\theta J = \frac{1}{m}X^T(\hat{p} - y)$

âœ… è¯´æ˜ï¼š
	â€¢	MSE åœ¨ sigmoid æƒ…å†µä¸‹æ”¶æ•›æ…¢ï¼ŒCross-Entropy æ˜¯æ ‡å‡†å½¢å¼ï¼›
	â€¢	æ¢¯åº¦ä¸çº¿æ€§å›å½’å½¢å¼ç›¸åŒï¼Œåªæ˜¯ $\hat{y}$ ä¸åŒï¼›
	â€¢	æŸå¤±æ˜¯å‡¸å‡½æ•°ï¼ŒGD å¯æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ã€‚

â¸»

ğŸ§© 5ï¸âƒ£ Ridge Logistic Regressionï¼ˆL2 æ­£åˆ™ Logisticï¼‰

ğŸ§® Loss

$J(\theta) = -\frac{1}{m}\sum_i [y_i \log(\hat{p_i}) + (1 - y_i)\log(1 - \hat{p_i})] + \frac{\lambda}{2m}\|\theta\|^2$

âš™ï¸ Gradient

$\nabla_\theta J = \frac{1}{m}X^T(\hat{p} - y) + \frac{\lambda}{m}\theta$

âœ… è¯´æ˜ï¼š
	â€¢	å¸¸ç”¨äºå·¥ä¸šå®è·µï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ï¼›
	â€¢	L2 æ­£åˆ™ä¸ä¼šè®©æƒé‡ä¸ºé›¶ï¼Œä½†èƒ½æ§åˆ¶è§„æ¨¡ï¼›
	â€¢	å¦‚æœå¸Œæœ›ç¨€ç–ï¼Œå¯ä»¥æ”¹æˆ L1ã€‚

â¸»

ğŸ§© 6ï¸âƒ£ Softmax Regressionï¼ˆå¤šåˆ†ç±» Logisticï¼‰

ğŸ§® Softmax å‡½æ•°

$\hat{p}{ik} = \frac{e^{z{ik}}}{\sum_j e^{z_{ij}}}$, $\quad z = XW$
å…¶ä¸­ï¼š
	â€¢	W çš„å½¢çŠ¶æ˜¯ (n_features, n_classes)
	â€¢	\hat{p}_{ik} æ˜¯æ ·æœ¬ i å±äºç±»åˆ« k çš„æ¦‚ç‡

ğŸ§® Lossï¼ˆå¤šç±» Cross-Entropyï¼‰

$J(W) = -\frac{1}{m}\sum_i \sum_k y_{ik}\log(\hat{p}{ik})$
å…¶ä¸­ $y{ik}$ æ˜¯ one-hot labelã€‚

âš™ï¸ Gradient

$\nabla_W J = \frac{1}{m}X^T(\hat{P} - Y)$

âœ… è¯´æ˜ï¼š
	â€¢	è·Ÿ logistic regression å‡ ä¹ä¸€æ ·ï¼Œåªæ˜¯ sigmoidâ†’softmaxï¼›
	â€¢	Y æ˜¯ one-hot çŸ©é˜µï¼›
	â€¢	è¿™å°±æ˜¯ç¥ç»ç½‘ç»œæœ€åä¸€å±‚ softmax çš„æ¢¯åº¦å…¬å¼ã€‚

â¸»

ğŸ§© 7ï¸âƒ£ (Bonus) Poisson Regressionï¼ˆè®¡æ•°å‹ GLMï¼‰

ğŸ§® å‡è®¾

$y_i \sim \text{Poisson}(\lambda_i), \quad \lambda_i = e^{X_i\theta}$

ğŸ§® Lossï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰

$J(\theta) = \frac{1}{m}\sum_i [e^{X_i\theta} - y_i(X_i\theta)]$

âš™ï¸ Gradient

$\nabla_\theta J = \frac{1}{m}X^T(e^{X\theta} - y)$

âœ… è¯´æ˜ï¼š
	â€¢	åˆæ˜¯åŒæ ·çš„å½¢å¼ï¼š $X^T(\hat{y}-y)$ï¼Œåªä¸è¿‡ $\hat{y} = e^{X\theta}$

![img.png](img.png)

âœ… ä¹ã€ä¸€å¥è¯æ€»ç»“

æ‰€æœ‰è¿™äº›æ¨¡å‹éƒ½å±äº å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰ æ¡†æ¶ï¼š
g(\mathbb{E}[y|x]) = X\theta
å®ƒä»¬çš„æ¢¯åº¦éƒ½å…·æœ‰ç±»ä¼¼çš„ç»“æ„ï¼š
\nabla_\theta J = X^T(\hat{y} - y) + \text{(regularization term)}
å”¯ä¸€åŒºåˆ«åœ¨äºï¼š
	â€¢	link functionï¼ˆlinear / sigmoid / softmax / expï¼‰
	â€¢	loss çš„å½¢å¼ï¼ˆMSE / Cross-Entropy / NLLï¼‰
	â€¢	æ­£åˆ™åŒ–é¡¹ï¼ˆL1/L2ï¼‰


# ğŸ™‹1 GRADIENT DESCENT TYPES
========================

There are three main variants of Gradient Descent used in optimization:

1ï¸âƒ£ Batch Gradient Descent (BGD)
--------------------------------
- Uses the **entire training dataset** to compute the gradient at each step.
- Update rule: Î¸ â† Î¸ - Î± * (1/m) * Xáµ€(XÎ¸ - y)
- Pros:
    â€¢ Produces an exact gradient (no noise)
    â€¢ Converges smoothly and stably
- Cons:
    â€¢ Very slow for large datasets (one update per epoch)
    â€¢ Requires full data to fit in memory

2ï¸âƒ£ Stochastic Gradient Descent (SGD)
-------------------------------------
- Uses **a single random sample** to compute the gradient and update parameters.
- Update rule: Î¸ â† Î¸ - Î± * âˆ‡J(xáµ¢, yáµ¢)
- Pros:
    â€¢ Fast updates (one per sample)
    â€¢ Can escape shallow local minima due to noise
    â€¢ Suitable for online learning
- Cons:
    â€¢ Noisy convergence (loss oscillates)
    â€¢ Sensitive to learning rate
    â€¢ May never fully converge, only fluctuate around the optimum

3ï¸âƒ£ Mini-Batch Gradient Descent (MBGD)
--------------------------------------
- Uses a **small random subset of samples** (e.g., 32, 64, 128) per update.
- Update rule: Î¸ â† Î¸ - Î± * (1/batch_size) * X_batcháµ€(X_batchÎ¸ - y_batch)
- Pros:
    â€¢ Trade-off between speed and stability
    â€¢ Works efficiently on GPUs (vectorized computation)
    â€¢ Most commonly used in deep learning and large-scale ML
- Cons:
    â€¢ Requires tuning of batch size
    â€¢ Still introduces small gradient noise

ğŸ§  Summary Table
--------------------------------------
| Method | Gradient Computed From | Noise | Convergence | Speed | Use Case |
|---------|------------------------|--------|--------------|--------|-----------|
| Batch | All samples | None | Very stable | Slow | Small datasets |
| SGD | One sample | High | Noisy | Fast per step | Online / streaming data |
| Mini-Batch | Small subset | Moderate | Smooth | Fast + stable | Deep learning standard |

âš™ï¸ Practical Tip
--------------------------------------
In modern ML systems, "SGD" almost always means **Mini-Batch SGD** â€” 
each iteration uses a small random batch to approximate the full gradient.

All three follow the same core update rule:
    Î¸ â† Î¸ - Î± * âˆ‡J(Î¸)
but differ in how the gradient is estimated per iteration.
"""

# ğŸ™‹ 2 Principal Component Analysis (PCA) â€” Full Summary

## 1ï¸âƒ£ Problem Definition
Given a data matrix  
$X \in \mathbb{R}^{m \times n}$
where:
- \(m\) = number of samples  
- \(n\) = number of features  

**Goal:** Find a low-dimensional representation $(Z \in \mathbb{R}^{m \times k}) (where (k < n))$ that preserves as much variance (information) as possible.

We seek a transformation matrix:
$U_k \in \mathbb{R}^{n \times k}$
such that:
$Z = X_c U_k$
where \(X_c\) is the **centered** data (zero-mean).

---

## 2ï¸âƒ£ Center the Data
Since variance is computed around the mean, first subtract the column-wise mean:
$X_c = X - \mathbf{1}\mu^\top,\qquad \mu = \frac{1}{m}\sum_{i=1}^m x_i$

---

## 3ï¸âƒ£ Optimization Objective
Find a unit vector \(u\) that maximizes the **variance** of the data projected onto \(u\):
$\max_{u}\ \mathrm{Var}(X_c u)\quad \text{s.t. } \|u\|=1$

For each data point \(x_i\), its projection onto \(u\) is a scalar:
$z_i = x_i^\top u$

Since the data are centered $(\bar z = 0)$:
$\mathrm{Var}(z) = \frac{1}{m}\sum_{i=1}^m (x_i^\top u)^2$

Matrix form:
$\mathrm{Var}(z) = \frac{1}{m}u^\top X_c^\top X_c\, u \;=\; u^\top \Sigma u$
with the covariance matrix:
$\Sigma = \frac{1}{m}\, X_c^\top X_c$

---

## 4ï¸âƒ£ Lagrange Multiplier â‡’ Eigenvalue Problem
Maximize $(u^\top \Sigma u)$ subject to $(\|u\|=1)$:
$\mathcal{L}(u,\lambda) = u^\top \Sigma u  - \lambda (u^\top u - 1)$
Set derivative to zero:
$2\Sigma u - 2\lambda u = 0 \quad\Rightarrow\quad \Sigma u = \lambda u$

**Therefore:**
- $(u_i)$: eigenvectors of \(\Sigma\) (principal component directions)  
- $(\lambda_i)$: eigenvalues = variance explained by each component  

Sort eigenvalues in descending order, take the top \(k\) eigenvectors â†’ $(U_k)$.

---

## 5ï¸âƒ£ Dimensionality Reduction & Reconstruction
- **Projection:**
  $Z = X_c U_k$
- **Reconstruction:**
  $\hat{X} = Z U_k^\top + \mu$

---

## 6ï¸âƒ£ Why Use SVD Instead of Eigen Decomposition
Directly computing eigenvectors of \(\Sigma = \frac{1}{m} X_c^\top X_c\) (shape \(n \times n\)) can be unstable or expensive when \(n\) is large.

Instead, perform **SVD**:
$X_c = U\, S\, V^\top$
Then:
$X_c^\top X_c \;=\; V\, S^2\, V^\top$

Hence:
- **Principal directions:** columns of \(V\)  
- **Variance per component:** \(S_i^2/(m-1)\)  
- **Explained variance ratio:**  
  $r_i \;=\; \frac{S_i^2}{\sum_j S_j^2}$

---

## 7ï¸âƒ£ Selecting the Number of Components (k)
Cumulative explained variance ratio:
$R_k \;=\; \frac{\sum_{i=1}^{k} S_i^2}{\sum_{i=1}^{r} S_i^2}$

Choose the smallest \(k\) such that \(R_k \ge 0.95\).

```python
svals = S  # singular values from SVD
explained_var_ratio = svals**2 / np.sum(svals**2)
cum_ratio = np.cumsum(explained_var_ratio)
k = np.searchsorted(cum_ratio, 0.95) + 1
```

---
## 8ï¸âƒ£ Minimal PCA (SVD) Implementation
```
python
import numpy as np

# 1) Center
Xc = X - X.mean(axis=0)

# 2) SVD
U, S, Vt = np.linalg.svd(Xc, full_matrices=False)

# 3) Explained variance ratio
explained_var_ratio = (S**2) / np.sum(S**2)

# 4) Choose k (e.g., 95%)
cum_ratio = np.cumsum(explained_var_ratio)
k = np.searchsorted(cum_ratio, 0.95) + 1

# 5) Project
V_k = Vt[:k, :].T       # (n, k)
Z = Xc @ V_k            # (m, k)

# (Optional) Reconstruct
X_rec = Z @ V_k.T + X.mean(axis=0)
```

## 9ï¸âƒ£ Geometric Intuition
	â€¢	Principal components are orthogonal axes pointing along directions of maximum variance.
	â€¢	Components are uncorrelated.
	â€¢	Early components capture most information; later ones are often noise.
---

## ğŸ”Ÿ Key Takeaways (Cheat Sheet)
| Concept | Expression | Meaning |
|----------|-------------|----------|
| Projection of a point | $z_i = x_i^\top u$ | Coordinate of \(x_i\) along direction \(u\) |
| Variance along \(u\) | $\mathrm{Var}(z) = u^\top \Sigma u$ | How widely data spread along \(u\) |
| Optimization | $\max_u u^\top \Sigma u \ \text{s.t.}\ \|u\|=1$ | Find the most informative direction |
| Eigen equation | $\Sigma u = \lambda u$ | \(u\): direction; \(\lambda\): variance |
| SVD link | $X_c = U S V^\top$ | \(V\) gives principal directions |
| Variance from SVD | $S_i^2/(m-1)$ | Variance explained by component \(i\) |
| Explained var. ratio | $S_i^2 \Big/ \sum_j S_j^2$ | Fraction of total variance |
| Cumulative ratio | $\sum_{i\le k} S_i^2 \Big/ \sum_j S_j^2$ | Info kept by first \(k\) components |
| Projection matrix | $U_k \in \mathbb{R}^{n\times k}$ | New basis (orthonormal) |
| Low-dim data | $Z = X_c U_k$ | Shape \((m, k)\) |


# ğŸ§  Machine Learning Model Evaluation Summary
_From Cross Validation â†’ Grid Search â†’ Biasâ€“Variance Tradeoff_

---

## ğŸ§© 1. Cross Validation (CV)

### ğŸ’¡ What is CV?

Cross Validation is a **model evaluation technique** to estimate a modelâ€™s ability to **generalize to unseen data**.

Instead of relying on a single train/test split, CV rotates multiple â€œtrainâ€“validationâ€ partitions
so that every sample gets a chance to be used for validation.

---

### âš™ï¸ K-Fold Cross Validation

**Steps:**

1. Shuffle the data.
2. Split into `K` equally sized folds.
3. For each fold `i`:
   - Train on `Kâˆ’1` folds.
   - Validate on the remaining fold.
   - Record the validation metric.
4. Compute the average score across K folds.

$[ E_{cv} = \frac{1}{K} \sum_{i=1}^{K} L(f_{\theta^{(i)}}(val_i))]$

---

### ğŸ§® Example (K = 3)

| Iteration | Train Folds | Validation Fold |
|------------|-------------|----------------|
| 1 | Fold2 + Fold3 | Fold1 |
| 2 | Fold1 + Fold3 | Fold2 |
| 3 | Fold1 + Fold2 | Fold3 |

The final score = mean of 3 validation results.

---

### ğŸ§© Leave-One-Out (LOO-CV)

- Special case of K-Fold where `K = N` (N = number of samples).
- Each iteration:
  - Train on Nâˆ’1 samples.
  - Validate on the remaining one.

| Property | K-Fold | Leave-One-Out |
|-----------|---------|---------------|
| Validation size | N/K | 1 |
| Training size | NÃ—(Kâˆ’1)/K | Nâˆ’1 |
| Training rounds | K | N |
| Bias | Slightly higher | Lowest |
| Variance | Lower | Highest |
| Computation | Moderate | Expensive |

---

### âš ï¸ Common Mistakes (Data Leakage)

âŒ Wrong:
```python
# Normalizing before split (leaks info)
X = (X - X.mean()) / X.std()
```

Correct:
```
# Normalize inside each fold
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)
```

### ğŸ§­ When to Use Different CVs

| Task | Recommended CV | Explanation |
|------|----------------|-------------|
| **Classification** | StratifiedKFold | Ensures each fold preserves label distribution (e.g., class balance). |
| **Regression** | KFold | Regular K-Fold works since outputs are continuous. |
| **Time Series** | TimeSeriesSplit | Keeps chronological order â€” training only on past to predict future. |
| **Very Small Dataset** | Leave-One-Out (LOO) | Maximizes training size; each sample validated once. |
| **Grouped or Hierarchical Data** | GroupKFold | Ensures samples from the same group never appear in both train and validation. |

---

### ğŸ§  Biasâ€“Variance Effect of K

| K Value | Bias | Variance | Notes |
|----------|------|-----------|-------|
| Small (e.g., 3) | High Bias | Low Variance | Coarse estimate, faster. |
| Large (e.g., 10) | Low Bias | High Variance | More stable, slower. |
| Leave-One-Out | Lowest Bias | Highest Variance | Best use of data, but very slow. |

- Larger K means training sets are bigger (â†’ lower bias),  
  but validation sets are smaller (â†’ higher variance).  
- Leave-One-Out (K=N) gives the most unbiased estimate, but has high variance and computational cost.

---

## âš™ï¸ 2. Grid Search + Cross Validation

### ğŸ’¡ Purpose

Grid Search systematically **explores hyperparameter combinations**,  
while Cross Validation **evaluates** each combinationâ€™s generalization performance.

```python
for Î¸ in parameter_grid:
    mean_score = average_of_Kfold(model(Î¸))
select Î¸* = argmax(mean_score)
```

### ğŸ”¹ How They Work Together

| Component | Role | Relationship |
|------------|------|--------------|
| **Grid Search** | Adjusts model hyperparameters | Controls model complexity and Biasâ€“Variance tradeoff |
| **Cross Validation** | Evaluates each configuration | Estimates model stability and generalization variance |

---

### ğŸ§© Example: Ridge Regression

$[J(\theta) = \text{MSE} + \lambda \sum_i \theta_i^2 ]$

| Î» (Regularization) | Model Complexity | Bias | Variance |
|---------------------|------------------|-------|-----------|
| Large Î» | Simpler model | â†‘ | â†“ |
| Small Î» | More flexible | â†“ | â†‘ |

CV evaluates which Î» yields the **lowest validation error**,  
achieving the best balance between bias and variance.

---

### ğŸ“Š Example Result

| Î» | Mean CV Error | Std (Variance Indicator) |
|---|----------------|--------------------------|
| 0.001 | 0.12 | Â±0.09 |
| 0.01  | 0.09 | Â±0.05 |
| 0.1   | 0.07 âœ… | Â±0.03 |
| 1.0   | 0.09 | Â±0.02 |
| 10.0  | 0.15 | Â±0.01 |

âœ… **Î» = 0.1** gives the lowest mean validation error and acceptable variance.

---

## ğŸ§  3. Biasâ€“Variance Tradeoff

### ğŸ§® Formula

$[E[(y - \hat{f}(x))^2] = Bias^2 + Variance + Irreducible\ Noise]$

| Term | Meaning |
|------|----------|
| **BiasÂ²** | Systematic error â€” model too simple (underfitting). |
| **Variance** | Sensitivity to training data â€” model too complex (overfitting). |
| **Irreducible Noise** | Random data noise â€” canâ€™t be reduced. |

---

### ğŸ¯ Intuitive Analogy (Target Board ğŸ¯)

| Pattern | Bias | Variance | Description |
|----------|------|-----------|-------------|
| Darts far from center but close together | High | Low | Underfitting |
| Darts near center but scattered | Low | High | Overfitting |
| Darts tight & near center | Low | Low | Ideal |

---

### ğŸ“ˆ Tradeoff Curve

```
Error
^
|
| \            Total Error
|  \        /
|   \      /
|    \    /
|     \  /
|      /    Variance
|      /\
|     /  \    Bias^2 
|â€“â€“/â€“â€“-â€”â€”â€”â€”â€”â€“> Model Complexity
```

- Left side â†’ Simple models: **High bias, low variance**  
- Right side â†’ Complex models: **Low bias, high variance**  
- Middle region â†’ **Best tradeoff, lowest total error**

---

### âœ… Practical Guide

| Scenario | Bias | Variance | Recommended Action |
|-----------|------|-----------|--------------------|
| Underfitting | â†‘ | â†“ | Use a more complex model / reduce regularization |
| Overfitting | â†“ | â†‘ | Simplify model / add regularization / use more data |
| Balanced | Moderate | Moderate | Ideal generalization |

---

## ğŸ“Š 4. Classification Metrics

### ğŸ”¹ Basic Terms

| Term | Meaning |
|------|----------|
| **TP** | True Positive |
| **FP** | False Positive |
| **FN** | False Negative |
| **TN** | True Negative |

---

### ğŸ”¸ Common Metrics

| Metric | Formula | Interpretation |
|---------|----------|----------------|
| **Accuracy** | (TP + TN) / (TP + TN + FP + FN) | Overall correctness |
| **Precision** | TP / (TP + FP) | Of predicted positives, how many are correct |
| **Recall (Sensitivity)** | TP / (TP + FN) | Of actual positives, how many were found |
| **F1 Score** | 2Ã—(PrecisionÃ—Recall)/(Precision+Recall) | Balance between precision & recall |
| **AUC (ROC)** | Area under ROC curve | Overall ranking ability of classifier |

---

### âš–ï¸ Precisionâ€“Recall Tradeoff

| Model Behavior | Recall | Precision | Example Use Case |
|----------------|--------|------------|------------------|
| Predicts most positives | High | Low | Disease detection, fraud detection |
| Predicts cautiously | Low | High | Spam filters, recommendation systems |
| Balanced | Moderate | Moderate | General classification tasks |

---

### ğŸ§© Choosing the Right Metric

| Scenario | Goal | Recommended Metric |
|-----------|------|--------------------|
| Disease / Fraud detection | Avoid missing true positives | **Recall** |
| Spam / Ad click prediction | Avoid false alarms | **Precision** |
| Balanced classification | Balance both errors | **F1 Score** |
| Ranking / Probabilistic models | Evaluate discrimination power | **AUC (ROC)** |

---

## ğŸ§­ 5. Summary Table

| Concept | Purpose | Affects | Notes |
|----------|----------|----------|-------|
| **Cross Validation** | Evaluate model generalization on training data | Estimates Bias & Variance | Internal validation mechanism |
| **Grid Search** | Tune hyperparameters systematically | Controls Biasâ€“Variance tradeoff | Uses CV for evaluation |
| **Bias** | Systematic error â€” model too simple | â†“ with complexity | High bias â†’ underfitting |
| **Variance** | Sensitivity to data â€” model too complex | â†‘ with complexity | High variance â†’ overfitting |
| **Metrics** | Quantify model performance | Depends on task | Precision/Recall/F1/AUC for classification; MSE/RÂ² for regression |

---

## ğŸ§  Interview Tip

If asked:

> â€œWhatâ€™s the relationship between Cross Validation, Grid Search, and Biasâ€“Variance Tradeoff?â€

You can say:

> â€œGrid Search adjusts model complexity (thus controlling biasâ€“variance).  
> Cross Validation evaluates each configurationâ€™s stability and generalization (revealing variance across folds).  
> Together, they help select the hyperparameters that minimize total generalization error.â€

---

âœ… **In one sentence:**

> Cross Validation estimates performance,  
> Grid Search tunes complexity,  
> and Biasâ€“Variance Tradeoff explains *why* their balance determines generalization.


## Cross entropy loss
ğŸ§© ä¸€ã€Cross Entropy æ˜¯ä¸ªâ€œå¤§æ¡†æ¶â€

Cross Entropyï¼ˆäº¤å‰ç†µï¼‰æ˜¯ä¸€ä¸ªé€šç”¨çš„åˆ†å¸ƒå·®å¼‚åº¦é‡å‡½æ•°ï¼š

$H(p, q) = -\sum_i q_i \log(p_i)$

	â€¢	q_iï¼šçœŸå®åˆ†å¸ƒï¼ˆçœŸå®æ ‡ç­¾ one-hotï¼‰
	â€¢	p_iï¼šæ¨¡å‹é¢„æµ‹åˆ†å¸ƒï¼ˆç»è¿‡ softmax / sigmoidï¼‰

å®ƒè¡¡é‡ï¼š

æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ p è·ç¦»çœŸå®åˆ†å¸ƒ q æœ‰å¤šè¿œã€‚
å½“ p=q æ—¶ï¼Œäº¤å‰ç†µæœ€å°ã€‚


## geometric median
ğŸ§© ä¸€ã€é¢˜ç›®ç†è§£ä¸èƒŒæ™¯

ç»™å®šäºŒç»´ç©ºé—´ä¸­ N ä¸ªç‚¹ï¼š
$P = \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$

æ±‚ä¸€ä¸ªç‚¹ Q = (x, y)ï¼Œ
ä½¿å¾—å®ƒåˆ°æ‰€æœ‰ç‚¹çš„ æ¬§å‡ é‡Œå¾—è·ç¦»ä¹‹å’Œæœ€å°ï¼š
$f(x, y) = \sum_{i=1}^{N} \sqrt{(x - x_i)^2 + (y - y_i)^2}$

è¿™ä¸ªç‚¹å«åš å‡ ä½•ä¸­ä½æ•° (Geometric Median)ã€‚

â¸»

âš ï¸ äºŒã€ä¸ºä»€ä¹ˆä¸èƒ½ç”¨ K-Means çš„å‡å€¼æ›´æ–°ï¼Ÿ

K-Means çš„ä¼˜åŒ–ç›®æ ‡æ˜¯ï¼š
$J(c) = \sum_i (x_i - c)^2$
å®ƒæ˜¯ å¹³æ–¹è·ç¦» (L2 norm squared)ï¼Œ
æ±‚å¯¼åå¾—åˆ°çº¿æ€§æ–¹ç¨‹ï¼š
$\frac{\partial J}{\partial c} = -2\sum_i (x_i - c) = 0$
äºæ˜¯é—­å¼è§£ä¸ºï¼š
$c = \frac{1}{N}\sum_i x_i$

â¸»

ä½†æ˜¯å‡ ä½•ä¸­ä½æ•°çš„ç›®æ ‡å‡½æ•°æ˜¯ï¼š
$J(c) = \sum_i \sqrt{(x_i - c)^2}$

å¯¹ c æ±‚å¯¼ï¼š
$\frac{\partial J}{\partial c} = \sum_i \frac{c - x_i}{\|c - x_i\|}$

è¿™æ—¶ c å‡ºç°åœ¨åˆ†æ¯ä¸­ â†’ éçº¿æ€§æ–¹ç¨‹
æ²¡æœ‰é—­å¼è§£ï¼Œåªèƒ½ç”¨ è¿­ä»£æ³• æ±‚è¿‘ä¼¼ã€‚

â¸»

âš™ï¸ ä¸‰ã€Weiszfeldâ€™s Algorithm

Weiszfeld ç®—æ³•æ˜¯å‡ ä½•ä¸­ä½æ•°çš„æ ‡å‡†è¿­ä»£æ±‚è§£æ³•ã€‚

æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š

$c^{(t+1)} = \frac{\sum_i w_i x_i}{\sum_i w_i}, \quad  \text{å…¶ä¸­ } w_i = \frac{1}{\|x_i - c^{(t)}\|}$

é€»è¾‘è§£é‡Šï¼š
	â€¢	ç¦»å½“å‰ç‚¹ è¿‘ çš„æ ·æœ¬æƒé‡ æ›´é«˜ï¼›
	â€¢	ç¦»å½“å‰ç‚¹ è¿œ çš„æ ·æœ¬æƒé‡ æ›´ä½ï¼›
	â€¢	æ¯æ¬¡æ ¹æ®è¿™äº›æƒé‡é‡æ–°åŠ æƒå¹³å‡ï¼›
	â€¢	å½“æ–°æ—§ c çš„å·®è·å¾ˆå°æ—¶ï¼Œåœæ­¢è¿­ä»£ã€‚